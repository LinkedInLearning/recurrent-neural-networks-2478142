{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc90f29",
   "metadata": {},
   "source": [
    "## 07.01. Loading Spam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6332e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded Data :\n",
      "------------------------------------\n",
      "  CLASS                                                SMS\n",
      "0   ham   said kiss, kiss, i can't do the sound effects...\n",
      "1   ham      &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
      "2  spam  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
      "3  spam  * FREE* POLYPHONIC RINGTONE Text SUPER to 8713...\n",
      "4  spam  **FREE MESSAGE**Thanks for using the Auction S...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "#Load Spam Data and review content\n",
    "spam_data = pd.read_csv(\"Spam-Classification.csv\")\n",
    "\n",
    "print(\"\\nLoaded Data :\\n------------------------------------\")\n",
    "print(spam_data.head())\n",
    "\n",
    "#Separate feature and target data\n",
    "spam_classes_raw = spam_data[\"CLASS\"]\n",
    "spam_messages = spam_data[\"SMS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e745a",
   "metadata": {},
   "source": [
    "## 07.02. Preprocessing Spam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e937480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot Encoding Shape :  (1500, 2)\n"
     ]
    }
   ],
   "source": [
    "#Build a label encoder for target variable to convert strings to numeric values.\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "spam_classes = label_encoder.fit_transform(\n",
    "                                spam_classes_raw)\n",
    "\n",
    "#Convert target to one-hot encoding vector\n",
    "spam_classes = tf.keras.utils.to_categorical(spam_classes,2)\n",
    "\n",
    "print(\"One-hot Encoding Shape : \", spam_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a66f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens found:  4688\n",
      "Example token ID for word \"me\" : 25\n",
      "\n",
      "Total sequences found :  1500\n",
      "Example Sequence for sentence :   said kiss, kiss, i can't do the sound effects! He is a gorgeous man isn't he! Kind of person who needs a smile to brighten his day! \n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  260  921  921    4  430   55    6 1488 2294  148   10\n",
      "    3 1489  464 1143  148  922   19  514   77 1144    3  515    1 2295\n",
      "  397   89]\n"
     ]
    }
   ],
   "source": [
    "#Preprocess data for spam messages\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Max words in the vocabulary for this dataset\n",
    "VOCAB_WORDS=10000\n",
    "#Max sequence length for word sequences\n",
    "MAX_SEQUENCE_LENGTH=100\n",
    "\n",
    "#Create a vocabulary with unique words and IDs\n",
    "spam_tokenizer = Tokenizer(num_words=VOCAB_WORDS)\n",
    "spam_tokenizer.fit_on_texts(spam_messages)\n",
    "\n",
    "\n",
    "print(\"Total unique tokens found: \", len(spam_tokenizer.word_index))\n",
    "print(\"Example token ID for word \\\"me\\\" :\", spam_tokenizer.word_index.get(\"me\"))\n",
    "\n",
    "#Convert sentences to token-ID sequences\n",
    "spam_sequences = spam_tokenizer.texts_to_sequences(spam_messages)\n",
    "\n",
    "#Pad all sequences to fixed length\n",
    "spam_padded = pad_sequences(spam_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"\\nTotal sequences found : \", len(spam_padded))\n",
    "print(\"Example Sequence for sentence : \", spam_messages[0] )\n",
    "print(spam_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93cf70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                                    spam_padded,spam_classes,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a2a40",
   "metadata": {},
   "source": [
    "## 07.03. Building the embeddding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea38967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Size:  400000\n",
      "\n",
      " Sample Dictionary Entry for word \"the\" :\n",
      " [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "#Load the pre-trained embeddings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Read pretrained embeddings into a dictionary\n",
    "glove_dict = {} \n",
    "\n",
    "#Loading a 50 feature (dimension) embedding with 6 billion words\n",
    "with open('glove.6B.50d.txt', \"r\", encoding=\"utf8\") as glove_file:     \n",
    "    for line in glove_file:\n",
    "        \n",
    "        emb_line = line.split()      \n",
    "        emb_token = emb_line[0]         \n",
    "        emb_vector = np.array(emb_line[1:], dtype=np.float32)\n",
    "        \n",
    "        if emb_vector.shape[0] == 50:    \n",
    "            glove_dict[emb_token] = emb_vector \n",
    "\n",
    "print(\"Dictionary Size: \", len(glove_dict))\n",
    "print(\"\\n Sample Dictionary Entry for word \\\"the\\\" :\\n\", glove_dict.get(\"the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da915726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Embedding matrix : (4689, 50)\n",
      "Embedding Vector for word \"me\" : \n",
      " [-0.14524999  0.31265     0.15184    -0.63708001  0.63552999 -0.50295001\n",
      " -0.23214     0.52891999 -0.58629     0.53934997 -0.3055      1.03569996\n",
      " -0.77989    -0.19386999  1.22150004  0.24521001  0.26144001  0.22439\n",
      "  0.15583999 -0.79145998 -0.65262002  1.3211      0.76617998  0.38234001\n",
      "  1.44529998 -2.26430011 -1.15050006  0.50373     1.2651     -1.59029996\n",
      "  3.05180001  0.84118003 -0.69542998  0.29984999 -0.49151    -0.22312\n",
      "  0.59527999 -0.076347    0.52358001 -0.50133997  0.22483     0.01546\n",
      " -0.088005    0.21281999  0.28545001 -0.15976    -0.16777    -0.50895\n",
      "  0.14322001  1.01180005]\n"
     ]
    }
   ],
   "source": [
    "#We now associate each token ID in our data set vocabulary to the corresponding embedding in Glove\n",
    "#If the word is not available, then embedding will be all zeros.\n",
    "\n",
    "#Matrix with 1 row for each word in the data set vocubulary and 50 features\n",
    "\n",
    "vocab_len = len(spam_tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_len, 50))\n",
    "\n",
    "for word, id in spam_tokenizer.word_index.items():  \n",
    "    try:\n",
    "        embedding_vector = glove_dict.get(word) \n",
    "        if embedding_vector is not None:         \n",
    "            embedding_matrix[id] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Size of Embedding matrix :\", embedding_matrix.shape)\n",
    "print(\"Embedding Vector for word \\\"me\\\" : \\n\", embedding_matrix[spam_tokenizer.word_index.get(\"me\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef5aff",
   "metadata": {},
   "source": [
    "## 07.04. Build the Spam Model with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3793aa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Embedding-Layer (Embedding)  (None, 100, 50)          234450    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               314368    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 549,332\n",
      "Trainable params: 549,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Create a model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.layers import LSTM,Dense\n",
    "\n",
    "#Setup Hyper Parameters for building the model\n",
    "NB_CLASSES=2\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(vocab_len,\n",
    "                                 50, \n",
    "                                 name=\"Embedding-Layer\",\n",
    "                                 weights=[embedding_matrix],\n",
    "                                 input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                 trainable=True))\n",
    "\n",
    "#Add LSTM Layer\n",
    "model.add(LSTM(256))\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "                             name='Output-Layer',\n",
    "                             activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5e339d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Progress:\n",
      "------------------------------------\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 683ms/step - loss: 0.6623 - accuracy: 0.6333 - val_loss: 0.3876 - val_accuracy: 0.8958\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.3043 - accuracy: 0.9146 - val_loss: 0.3739 - val_accuracy: 0.8083\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 554ms/step - loss: 0.2967 - accuracy: 0.8938 - val_loss: 0.1947 - val_accuracy: 0.9208\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 534ms/step - loss: 0.1558 - accuracy: 0.9542 - val_loss: 0.1679 - val_accuracy: 0.9292\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 545ms/step - loss: 0.1823 - accuracy: 0.9323 - val_loss: 0.1573 - val_accuracy: 0.9417\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 2s 557ms/step - loss: 0.1136 - accuracy: 0.9635 - val_loss: 0.1347 - val_accuracy: 0.9500\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 2s 551ms/step - loss: 0.1263 - accuracy: 0.9542 - val_loss: 0.1394 - val_accuracy: 0.9458\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 2s 571ms/step - loss: 0.0894 - accuracy: 0.9740 - val_loss: 0.1221 - val_accuracy: 0.9625\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 2s 557ms/step - loss: 0.1303 - accuracy: 0.9552 - val_loss: 0.1282 - val_accuracy: 0.9458\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 2s 541ms/step - loss: 0.0851 - accuracy: 0.9781 - val_loss: 0.1184 - val_accuracy: 0.9583\n",
      "\n",
      "Evaluation against Test Dataset :\n",
      "------------------------------------\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 0.2026 - accuracy: 0.9433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20259273052215576, 0.9433333277702332]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make it verbose so we can see the progress\n",
    "VERBOSE=1\n",
    "\n",
    "#Setup Hyper Parameters for training\n",
    "BATCH_SIZE=256\n",
    "EPOCHS=10\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "print(\"\\nTraining Progress:\\n------------------------------------\")\n",
    "\n",
    "history=model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
    "model.evaluate(X_test,Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8c3f7",
   "metadata": {},
   "source": [
    "## 07.05. Predicting Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aed75176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Output: [1 0]\n",
      "Prediction Classes are  ['spam' 'ham']\n"
     ]
    }
   ],
   "source": [
    "# Two input strings to predict\n",
    "input_str=[\"Unsubscribe send GET EURO STOP to 83222\",\n",
    "            \"Yup I will come over\"]\n",
    "\n",
    "#Convert to sequence using the same tokenizer as training\n",
    "input_seq = spam_tokenizer.texts_to_sequences(input_str)\n",
    "#Pad the input\n",
    "input_padded = pad_sequences(input_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#Predict using model\n",
    "prediction=np.argmax( model.predict(input_padded), axis=1 )\n",
    "print(\"Prediction Output:\" , prediction)\n",
    "\n",
    "#Print prediction classes\n",
    "print(\"Prediction Classes are \", label_encoder.inverse_transform(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc92c974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
